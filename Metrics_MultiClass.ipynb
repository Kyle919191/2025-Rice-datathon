{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "paIh5VIalwlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9ba2d0-3a93-4c6c-a6c9-aea2c9f885e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-55860d0a5620>:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  csv[\"sex\"] = csv['sex'].replace(['F','M'],[-1,1])\n",
            "<ipython-input-1-55860d0a5620>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  csv[\"sex\"] = csv['sex'].replace(['F','M'],[-1,1])\n",
            "<ipython-input-1-55860d0a5620>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  csv[\"IQ\"] = csv[\"IQ\"].fillna(csv[\"IQ\"].mean())\n",
            "<ipython-input-1-55860d0a5620>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  csv[\"education\"] = csv[\"education\"].fillna(csv[\"education\"].mean())\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "\n",
        "csv = None\n",
        "csv = pd.read_csv(\"Train_and_Validate_EEG.csv\", header=0)\n",
        "\n",
        "print(len(csv))\n",
        "\n",
        "# Drop the blank columns\n",
        "csv = csv.dropna(axis=1, how='all')\n",
        "\n",
        "# convert sex to binary encoding\n",
        "csv[\"sex\"] = csv['sex'].replace(['F','M'],[-1,1])\n",
        "\n",
        "# Mean Imputation\n",
        "csv[\"IQ\"] = csv[\"IQ\"].fillna(csv[\"IQ\"].mean())\n",
        "csv[\"education\"] = csv[\"education\"].fillna(csv[\"education\"].mean())\n",
        "\n",
        "# Make the X data\n",
        "all_columns = csv.columns\n",
        "x_columns = all_columns.drop(\"ID\").drop(\"main.disorder\").drop(\"specific.disorder\").drop(\"eeg.date\")\n",
        "X = csv[x_columns]\n",
        "\n",
        "# make the Y data\n",
        "y = csv[\"main.disorder\"]\n",
        "\n",
        "# IQ: normalize\n",
        "X.loc[:,\"IQ\"] = (X[\"IQ\"] - 100) / 15\n",
        "\n",
        "# # Use Quantile Transformer\n",
        "# quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=0, n_quantiles=int(len(X)/5))\n",
        "\n",
        "# for column_name in X.columns[4:]:\n",
        "#    X.loc[:,column_name] = quantile_transformer.fit_transform(X[column_name].values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split train and test\n",
        "X_train, X_test, y_train_, y_test_ = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train),columns=X.columns)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test),columns=X.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a RandomForestClassifier\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# modify y to take a number for each unique disorder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to your data\n",
        "y_train = y_train_.copy()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "\n",
        "y_test = y_test_.copy()\n",
        "y_test = le.transform(y_test)\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = pd.DataFrame(y_pred)"
      ],
      "metadata": {
        "id": "TwkHfN-mLKAp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform 5-fold cross-validation and obtain accuracy scores for each fold\n",
        "scores = cross_val_score(model, X_test, y_test, cv=5)\n",
        "\n",
        "# Compute the mean accuracy and the standard deviation\n",
        "accuracy = scores.mean()\n",
        "std_dev = scores.std()\n",
        "\n",
        "print(accuracy, std_dev)"
      ],
      "metadata": {
        "id": "3zF7MotI4blV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Create a multioutput logistic regression model\n",
        "model = MultiOutputClassifier(LogisticRegression(solver='lbfgs', max_iter=1000))\n",
        "\n",
        "# modify y to take a number for each unique disorder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to your data\n",
        "y_train = y_train_.copy()\n",
        "\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "\n",
        "# Create an instance of OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_data = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "\n",
        "# Convert the encoded data to a DataFrame\n",
        "y_train = pd.DataFrame(encoded_data.toarray())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = pd.DataFrame(y_pred)"
      ],
      "metadata": {
        "id": "iAU88pK4CbQq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform 5-fold cross-validation and obtain accuracy scores for each fold\n",
        "y_test = le.transform(y_test_)\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_data = encoder.fit_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Convert the encoded data to a DataFrame\n",
        "y_test = pd.DataFrame(encoded_data.toarray())\n",
        "\n",
        "scores = cross_val_score(model, X_test, y_test, cv=5)\n",
        "\n",
        "# Compute the mean accuracy and the standard deviation\n",
        "accuracy = scores.mean()\n",
        "std_dev = scores.std()\n",
        "\n",
        "print(accuracy, std_dev)"
      ],
      "metadata": {
        "id": "cyrh3H22dVkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fa53fa-95bb-403a-f9fe-c3b9500ac01d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1055052790346908 0.026718979148211957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create XGBoost classifier\n",
        "clf = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=100,\n",
        "    tree_method=\"auto\",\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "# modify y to take a number for each unique disorder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to your data\n",
        "y_train = y_train_.copy()\n",
        "\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "\n",
        "y_test = y_test_.copy()\n",
        "y_test = le.transform(y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "VJTlXKjU5YkZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(model, X_test, y_test, cv=5)\n",
        "\n",
        "# Compute the mean accuracy and the standard deviation\n",
        "accuracy = scores.mean()\n",
        "std_dev = scores.std()\n",
        "\n",
        "print(accuracy, std_dev)"
      ],
      "metadata": {
        "id": "b8w2qTGS6OZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULsUtPWdHL7T",
        "outputId": "40364925-6d5e-4594-8e07-60146332d921"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1D CNN\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# Reshape data for 1D CNN (we need 3D input shape for Conv1D: [samples, timesteps, features])\n",
        "X_train_cnn = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val_cnn = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(64, activation='relu'))\n",
        "cnn_model.add(Dense(len(y.unique()), activation='softmax'))  # For multi-class classification\n",
        "\n",
        "cnn_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the CNN model\n",
        "y_train = pd.get_dummies(y_train_)\n",
        "y_test = pd.get_dummies(y_test_)\n",
        "cnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate CNN model\n",
        "cnn_val_loss, cnn_val_acc = cnn_model.evaluate(X_val_cnn, y_test)\n",
        "print(f\"1D CNN - Accuracy: {cnn_val_acc}\")\n",
        "print(f\"1D CNN - Loss: {cnn_val_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNmRVS43DKxs",
        "outputId": "b2b8bf39-1978-489f-c5fb-e2d3c28f9b02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.1979 - loss: 1.9533\n",
            "Epoch 2/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.2614 - loss: 1.8815\n",
            "Epoch 3/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3131 - loss: 1.8242\n",
            "Epoch 4/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.3140 - loss: 1.7693\n",
            "Epoch 5/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.2996 - loss: 1.7606\n",
            "Epoch 6/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.3013 - loss: 1.7479\n",
            "Epoch 7/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.3099 - loss: 1.7355\n",
            "Epoch 8/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.3025 - loss: 1.7446\n",
            "Epoch 9/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3130 - loss: 1.7263\n",
            "Epoch 10/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3208 - loss: 1.6836\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3387 - loss: 1.8143\n",
            "1D CNN - Accuracy: 0.32421875\n",
            "1D CNN - Loss: 1.8256103992462158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Model\n",
        "# Reshape the data for Transformer (similar to 1D CNN data)\n",
        "X_train_transformer = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val_transformer = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Instantiate and fit the encoder on the training labels\n",
        "le = LabelEncoder()\n",
        "y_train = y_train_.copy()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "\n",
        "y_test = y_test_.copy()\n",
        "y_val_encoded = le.transform(y_test)\n",
        "\n",
        "# Build the Transformer model\n",
        "input_layer = Input(shape=(X_train_transformer.shape[1], 1))\n",
        "\n",
        "# Transformer Block\n",
        "x = LayerNormalization()(input_layer)\n",
        "x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "# Dense Layers\n",
        "x = Dense(64, activation='relu')(x)\n",
        "output_layer = Dense(len(y.unique()), activation='softmax')(x)  # Multi-class classification\n",
        "\n",
        "transformer_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "transformer_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the Transformer model\n",
        "transformer_model.fit(X_train_transformer, y_train_encoded, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate Transformer model\n",
        "transformer_val_loss, transformer_val_acc = transformer_model.evaluate(X_val_transformer, y_val_encoded)\n",
        "print(f\"Transformer - Accuracy: {transformer_val_acc}\")\n",
        "print(f\"Transformer - Loss: {transformer_val_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSnHcQnHHASG",
        "outputId": "7f375375-df47-4685-d9a9-a881e84447ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 5s/step - accuracy: 0.2507 - loss: 1.9444\n",
            "Epoch 2/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 5s/step - accuracy: 0.2951 - loss: 1.9384\n",
            "Epoch 3/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 5s/step - accuracy: 0.2716 - loss: 1.9342\n",
            "Epoch 4/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 4s/step - accuracy: 0.2698 - loss: 1.9283\n",
            "Epoch 5/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 4s/step - accuracy: 0.3099 - loss: 1.9221\n",
            "Epoch 6/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 4s/step - accuracy: 0.2918 - loss: 1.9176\n",
            "Epoch 7/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.2815 - loss: 1.9148\n",
            "Epoch 8/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 5s/step - accuracy: 0.2762 - loss: 1.9104\n",
            "Epoch 9/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 4s/step - accuracy: 0.3026 - loss: 1.9038\n",
            "Epoch 10/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 4s/step - accuracy: 0.3003 - loss: 1.8987\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.3200 - loss: 1.8914\n",
            "Transformer - Accuracy: 0.30078125\n",
            "Transformer - Loss: 1.8950459957122803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tU7WleJHD9KQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}